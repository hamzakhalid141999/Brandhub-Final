{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, NavigableString \n",
    "import unicodecsv as ucsv\n",
    "import re \n",
    "from selenium import webdriver\n",
    "import time \n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from os.path  import basename\n",
    "import os\n",
    "import FeatureVector\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "cd = FeatureVector.ColorDescriptor((8, 12, 3)) #initialize color descriptor for feature vector extraction of all products\n",
    "access_token=\"ya29.a0AfH6SMBc2gCWfdkSRZDwIGNS2mOD9sWrPlYTrc1v2WZfO8ZnfCumBqkKYKP8fNUZbUXTrzJOCOTxpL40OsSZS8UAdH4BxCIWHmBmvDjXHf2sTdXrFGI_6sHoq2K-MJtdE1ZCw8qhPNV9vz6WC50vZbUApQanyM1L1NijbvoHWTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollBrowser(url,productID): #for scrollable websites\n",
    "    browser2 = webdriver.Chrome()\n",
    "    browser2.get(url)\n",
    "    browser2.page_source\n",
    "    delay = 50 # seconds\n",
    "\n",
    "    #open browser, scroll till end and then start scraping\n",
    "    try:\n",
    "        browser2.find_element_by_class_name(\"infinite-scrolling\").click()\n",
    "        browser2.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        #browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        myElem = WebDriverWait(browser2, delay).until(EC.presence_of_element_located((By.ID, productID)))\n",
    "        print (\"Page is ready!\")\n",
    "    except TimeoutException:\n",
    "        return browser2\n",
    "        print (\"Loading took too much time!\")\n",
    "    return browser2\n",
    "\n",
    "def InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors):\n",
    "    document = {\"PId\":id_name,\n",
    "                \"PName\":product_name,\n",
    "                \"PPrice\":price,\n",
    "                \"OnSale\":sale,\n",
    "                \"SPrice\":sale_price,\n",
    "                \"Stock\":stock,\n",
    "                \"Color\":Color,\n",
    "                \"Link\":product_link,\n",
    "                \"Fabric\":Fabric,\n",
    "                \"Description\":Description,\n",
    "                \"Type\":Type,\n",
    "               \"ImageName\": images_arr,\n",
    "               \"featureVectors\":str(feature_vectors)}\n",
    "    id1=db_collection.insert_one(document) #insert document in mongodb\n",
    "    return id1\n",
    "    \n",
    "def StoreonGoogleDrive(name,folderID,path):\n",
    "    headers = {\"Authorization\": \"Bearer \"+ access_token}\n",
    "    para = {\n",
    "        \"name\": name, #name of image in PC\n",
    "        \"parents\" : [folderID] #specify which folder it should go to\n",
    "    }\n",
    "    files = {\n",
    "        'data': ('metadata', json.dumps(para), 'application/json; charset=UTF-8'),\n",
    "        'file': open(path, \"rb\")\n",
    "    }\n",
    "    r = requests.post(\n",
    "        \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\",\n",
    "        headers=headers,\n",
    "        files=files\n",
    "    )\n",
    "\n",
    "    jsonobj=json.loads(r.text) #get text from request which contains the path of image in google drive\n",
    "    image_path=\"https://drive.google.com/file/d/\"+jsonobj['id']+\"/view\" #path of image in google drive\n",
    "    return image_path\n",
    "\n",
    "def getDbCon(Brand,Gender):\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://zahra:passmongodb@cluster0.femwg.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "    db = client[Brand]\n",
    "    db_collection=db[Gender]\n",
    "    return db_collection\n",
    "                            \n",
    "\n",
    "def JdotScrape():\n",
    "    WomenScrape() #scrape women products\n",
    "    MenScrape() #scrape men products\n",
    "   \n",
    "    \n",
    "def JWomenScrape(): #function for scarping women products\n",
    "    i=1 #for product count\n",
    "    page_number=1\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    condition=True\n",
    "\n",
    "    #make connection with db for Jdot \n",
    "    db_collection=getDbCon('Jdot','Women')\n",
    "    while (condition):\n",
    "        url='https://www.junaidjamshed.com/womens/stitched.html'.format(page_number) #open J. link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers=soup.findAll(\"li\",{\"class\":\"item product product-item\"})\n",
    "        if (len(containers)==0): #page doesn't have anything meaning page doesn't exist\n",
    "            condition=False #break from loop because no more pages to scrape\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"product_image\"}) #access links of all products on page\n",
    "\n",
    "            for link in link_containers: #iterate through all links\n",
    "                print(\"Product#\"+str(product_count))\n",
    "\n",
    "                if (product_count>0):\n",
    "                    linkurl=link.a['href'] #access link\n",
    "                    html=urlopen(linkurl) #open link\n",
    "                    soup=BeautifulSoup(html, \"html.parser\") #use Beautiful Soup to parse\n",
    "                    print(\"Product Link: \"+linkurl)\n",
    "\n",
    "                    #initialize everything null before iterating\n",
    "                    stock=\"\"\n",
    "                    price=\"\"\n",
    "                    Color=\"\"\n",
    "                    Fabric=\"\"\n",
    "                    type1=\"\"\n",
    "                    Desc=\"\"\n",
    "                    Details=\"\"\n",
    "                    sale=\"\"\n",
    "                    sale_price=\"\"\n",
    "\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"price-box price-final_price\"})\n",
    "                    price=price_container[0].span.span.span.text.strip() #store price\n",
    "\n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"stock available\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].span.text.strip() #store if product is in stock or not\n",
    "\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"additional-attributes-wrapper table-wrapper\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        color=description_container[0].table.tbody.tr.td.text\n",
    "                        description=description_container[0].table.tbody #access description container\n",
    "\n",
    "                        k=1\n",
    "                        #extract individual components from description and store in relevant variables\n",
    "                        for tr in description.findAll('tr'): \n",
    "                            Info=tr.th.text\n",
    "                            if (Info.startswith('Color')):\n",
    "                                Color = tr.td.text #store color\n",
    "                            elif (Info.startswith('Product Category')):\n",
    "                                type1 = tr.td.text #xtore type\n",
    "                            elif (Info.startswith('Fabric')):\n",
    "                                Fabric = tr.td.text #store fabric\n",
    "\n",
    "\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product attribute overview\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        description=description_container[0].div.p #store description\n",
    "\n",
    "\n",
    "                        k=1\n",
    "                        #store description,collection, details depending on the html\n",
    "                        for br in description.findAll('br'):\n",
    "                            Details = br.nextSibling\n",
    "                            br.replace_with(\"\")\n",
    "                            k+=1\n",
    "\n",
    "                        Desc=description.text\n",
    "                        if (Desc.startswith('Collection')):\n",
    "                            k=1\n",
    "                        else:\n",
    "                            Desc=\"\"\n",
    "\n",
    "                        if (Details.startswith('Description')):\n",
    "                            k=1\n",
    "                        else:\n",
    "                            Details=\"\"\n",
    "\n",
    "                    images_arr=[] \n",
    "                    feature_vectors=[]\n",
    "                    image_container=soup.findAll('a', {'data-image':re.compile('.jpg')}) #access images\n",
    "\n",
    "                    for image in image_container: #iterate through images\n",
    "                        url=image['data-image']   #gets the image but with extra characters\n",
    "                        split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                        substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "\n",
    "                         #downloads file:\n",
    "                        response = requests.get(substring)\n",
    "                        file = open(\"JF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                        file.write(response.content) #write image in our file\n",
    "                        file.close()\n",
    "\n",
    "                        id_name=\"JF_\"+str(product_count)+\"_\" \n",
    "\n",
    "                        #stores files in google drive:\n",
    "                        name=\"JF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                        path=r\"C:\\Users\\dell\\n\" + name\n",
    "                        path=os.path.normpath('C:/Users/dell/'+name) \n",
    "\n",
    "\n",
    "                        image_path=StoreonGoogleDrive(name,\"14qncJNgoTFvVxjsmXF2bpCsop6XqQpy8\",path) #this function stores the image with that name from path on google drive\n",
    "                        #print(image_path)\n",
    "\n",
    "                        image = cv2.imread(name)\n",
    "                        features = cd.describe(image)\n",
    "                        feature_vectors.append([])\n",
    "                        feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "\n",
    "                        images_arr.append(image_path)\n",
    "                        image_count=image_count+1\n",
    "\n",
    "                    \n",
    "                    #store data in db\n",
    "                    id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "                    print(id1)\n",
    "                    image_count=1\n",
    "                product_count+=1\n",
    "                print(\"\\n\")\n",
    "\n",
    "            page_number+=1\n",
    "            \n",
    "def JMenScrape():\n",
    "    i=1 #for product count\n",
    "    page_number=1\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    condition=True\n",
    "\n",
    "\n",
    "    db_collection=getDbCon('Jdot','Men')\n",
    "\n",
    "\n",
    "    while (condition):\n",
    "\n",
    "\n",
    "\n",
    "        url= 'https://www.junaidjamshed.com/mens/kameez-shalwar.html?p={}'.format(page_number) #open J. link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers=soup.findAll(\"li\",{\"class\":\"item product product-item\"})\n",
    "        if (len(containers)==0): #page doesn't have shit meaning page doesn't exist\n",
    "            condition=False #break from loop because no more pages to scrape\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"product_image\"})\n",
    "\n",
    "            for link in link_containers:\n",
    "                print(\"Product#\"+str(product_count))\n",
    "\n",
    "                if (product_count>0):\n",
    "                    linkurl=link.a['href']\n",
    "                    html=urlopen(linkurl)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    print(\"Product Link: \"+linkurl)\n",
    "\n",
    "                   \n",
    "                     #initialize everything null before iterating\n",
    "                    stock=\"\"\n",
    "                    price=\"\"\n",
    "                    Color=\"\"\n",
    "                    Fabric=\"\"\n",
    "                    type1=\"\"\n",
    "                    Desc=\"\"\n",
    "                    Details=\"\"\n",
    "                    sale=\"\"\n",
    "                    sale_price=\"\"\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"price-box price-final_price\"})\n",
    "                 \n",
    "                    price=price_container[0].span.span.span.text.strip()\n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"stock available\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].span.text.strip()\n",
    "                   \n",
    "\n",
    "\n",
    "                    Color=\"\"\n",
    "                    Fabric=\"\"\n",
    "                    type1=\"\"\n",
    "                    Desc=\"\"\n",
    "                    Details=\"\"\n",
    "                    description=\"\"\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"additional-attributes-wrapper table-wrapper\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        color=description_container[0].table.tbody.tr.td.text\n",
    "                        description=description_container[0].table.tbody\n",
    "\n",
    "                        k=1\n",
    "                        for tr in description.findAll('tr'):\n",
    "                            Info=tr.th.text\n",
    "                            if (Info.startswith('Color')):\n",
    "                                Color = tr.td.text #because fabric is the last tr(row) in table\n",
    "                            elif (Info.startswith('Product Category')):\n",
    "                                type1 = tr.td.text #because fabric is the last tr(row) in table\n",
    "                            elif (Info.startswith('Fabric')):\n",
    "                                Fabric = tr.td.text #because fabric is the last tr(row) in table\n",
    "\n",
    "\n",
    "                    description=\"\"\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product attribute overview\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        descript=description_container[0].findAll(\"div\",{\"class\":\"value\"})\n",
    "                        if (len(descript)!=0):\n",
    "                            description=descript[0].text\n",
    "                        else:\n",
    "                            description=\"\"\n",
    "\n",
    "       \n",
    "\n",
    "                    images_arr=[] \n",
    "                    feature_vectors=[]\n",
    "                    image_container=soup.findAll('a', {'data-image':re.compile('.jpg')})\n",
    "                    \n",
    "\n",
    "                    for image in image_container:\n",
    "                        url=image['data-image']   #gets the image but with extra characters\n",
    "                        split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                        substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "                       \n",
    "\n",
    "                  #downloads file:\n",
    "                        response = requests.get(substring)\n",
    "                        file = open(\"JM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                        file.write(response.content)\n",
    "                        file.close()\n",
    "\n",
    "                        id_name=\"JM_\"+str(product_count)+\"_\"\n",
    "\n",
    "                        #stores files in google drive:\n",
    "#                         name=\"JM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "                       \n",
    "#                         path=os.path.normpath('C:/Users/dell/'+name) \n",
    "                       \n",
    "                        \n",
    "#                         image_path=StoreonGoogleDrive(name,\"14qncJNgoTFvVxjsmXF2bpCsop6XqQpy8\",path) #this function stores the image with that name from path on google drive\n",
    "\n",
    "#                         image = cv2.imread(name)\n",
    "#                         features = cd.describe(image)\n",
    "#                         feature_vectors.append([])\n",
    "#                         feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "#                         images_arr.append(image_path)\n",
    "                        image_count=image_count+1\n",
    "\n",
    "\n",
    "\n",
    "#                     db_collection=getDbCon(Brand,'Men') #specify brand and gender to get db\n",
    "#                     #store data in db\n",
    "#                     id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                     print(id1)\n",
    "\n",
    "                    image_count=1\n",
    "                product_count+=1\n",
    "                print(\"\\n\")\n",
    "\n",
    "            page_number+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KhaadiScrape():\n",
    "    i=1 #for product count\n",
    "    image_count=1\n",
    "    page_number=1\n",
    "    product_count=1\n",
    "    count=1\n",
    "    condition=True\n",
    "\n",
    "    \n",
    "    db_collection=getDbCon('khaadi','Women')\n",
    "   \n",
    "    while (condition):\n",
    "        url= 'https://pk.khaadi.com/new-in.html?p={}'.format(page_number) #open khaadi link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers= soup.findAll(\"div\",{\"class\":\"products wrapper grid products-grid\"}) #grabs each product\n",
    "\n",
    "        if (len(containers)==0): #page doesn't have anything meaning page doesn't exist\n",
    "            condition=False #break from loop because no more pages to scrape\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"product-top\"})\n",
    "\n",
    "            type1=\"\"\n",
    "            id_name=\"\"\n",
    "            price=\"\"\n",
    "            stock=\"\"\n",
    "            sale=\"\"\n",
    "            sale_price=\"\"\n",
    "            Color=\"\"\n",
    "            material=\"\"\n",
    "            description=\"\"\n",
    "\n",
    "            for link in link_containers:\n",
    "                if (product_count>0):\n",
    "                    linkurl=link.a['href']\n",
    "                    html=urlopen(linkurl)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    print(\"Product Number: \"+str(product_count))\n",
    "                    print(\"Proudct Link: \"+linkurl)\n",
    "                    print(\"Proudct Name: \"+soup.title.text)\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"price-box price-final_price\"})\n",
    "                    price=price_container[0].span.span.span.next_element.next_element.next_element.next_element.text.strip()\n",
    "           \n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"product-info-stock-sku\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].div.span.text.strip()\n",
    "\n",
    "                    material_container=soup.findAll(\"div\",{\"class\":\"product-attribute-material\"})\n",
    "                    if len(material_container)!=0:\n",
    "                        material=material_container[0].span.next_element.next_element.text\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product-sub-infomation\"})\n",
    "                    if len(description_container)!=0:\n",
    "                        description=description_container[0].text.strip()\n",
    "\n",
    "                    image_container=soup.findAll('div', {'data-zoom':re.compile('.jpg')})\n",
    "\n",
    "                    type1=soup.title.text\n",
    "\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "\n",
    "                    for image in image_container:\n",
    "                        url=image['data-zoom']   #gets the image but with extra characters\n",
    "                        split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                        substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "                        #print(\"Image: \"+substring)\n",
    "\n",
    "                        #downloads file:\n",
    "                        response = requests.get(substring)\n",
    "                        file = open(\"KF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                        file.write(response.content)\n",
    "                        file.close()\n",
    "\n",
    "                        id_name=\"KF_\"+str(product_count)+\"_\"\n",
    "\n",
    "                        #stores files in google drive:\n",
    "#                         name=\"KF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "#                         #path=r\"C:\\Users\\dell\\n\" + name\n",
    "#                         path=os.path.normpath('C:/Users/dell/'+name) \n",
    "\n",
    "                        \n",
    "#                         image_path=StoreonGoogleDrive(name,\"10pDBHseIITjTloZXL4LxkDr4BsGZcF2e\",path) #this function stores the image with that name from path on google drive\n",
    "\n",
    "\n",
    "#                         image = cv2.imread(name)\n",
    "#                         features = cd.describe(image)\n",
    "#                         feature_vectors.append([])\n",
    "#                         feature_vectors[image_count-1]=features\n",
    "\n",
    "#                         images_arr.append(image_path)\n",
    "                        image_count=image_count+1\n",
    "                        \n",
    "                    \n",
    "#                     db_collection=getDbCon(Brand,Gender) #specify brand and gender to get db\n",
    "#                     #store data in db\n",
    "#                     id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                     print(id1)\n",
    "\n",
    "                image_count=1\n",
    "                product_count+=1\n",
    "                count+=1\n",
    "                print(\"\")\n",
    "                \n",
    "            page_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SapphireScrape():\n",
    "    i=1 #for product count\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    page_number=1\n",
    "    condition=True\n",
    "\n",
    "    \n",
    "    db_collection=getDbCon('sapphire','Women') #specify brand and gender to get db\n",
    "\n",
    "    while (condition):\n",
    "        url= 'https://pk.sapphireonline.pk/collections/ready-to-wear?p={}'.format(page_number) #open khaadi link, the format(page) inserts numbers in the page array into the url\n",
    "        html=urlopen(url)\n",
    "\n",
    "        soup=BeautifulSoup(html, \"html.parser\") #send link and tell format(2nd arg)\n",
    "\n",
    "        if (page_number==1):\n",
    "            title = soup.title\n",
    "            print(title.text,\"\\n\")\n",
    "\n",
    "        containers= soup.findAll(\"div\",{\"class\":\"product-collection products-grid row-bt\"}) #grabs each product\n",
    "\n",
    "        if (len(containers)==0): #page doesn't have anything meaning page doesn't exist\n",
    "            condition=False #break from loop because no more pages to scrape\n",
    "            print(\"\\nScraping Finished. Total Pages: \"+str(page_number-1))\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nPAGE#\"+str(page_number)) \n",
    "            container=containers[0]\n",
    "\n",
    "            link_containers=soup.findAll(\"div\",{\"class\":\"grid-item col-6 col-md-4 col-xl-3 four-columns\"})\n",
    "            print(\"linkk: \"+str(len(link_containers)))\n",
    "\n",
    "            for link in link_containers:\n",
    "                if (product_count>0 ):\n",
    "                    linkurl=\"https://pk.sapphireonline.pk/\" +link.a['href']\n",
    "                    print(\"Product# \"+str(product_count))\n",
    "                    print(\"Product Link: \"+linkurl)\n",
    "                    html=urlopen(linkurl)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    print(\"Product Name: \"+soup.h2.span.text)\n",
    "\n",
    "                    Fabric=\"\"\n",
    "                    Color=\"\"\n",
    "                    price=\"\"\n",
    "                    stock=\"\"\n",
    "                    type1=\"\"\n",
    "                    sale=\"\"\n",
    "                    sale_price=\"\"\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"prices\"})\n",
    "                    price=price_container[0].span.text.strip()\n",
    "                   # print(\"Price: \"+price)\n",
    "\n",
    "                    stock_container=soup.findAll(\"div\",{\"class\":\"product-inventory\"})\n",
    "                    if len(stock_container)!=0:\n",
    "                        stock=stock_container[0].span.text.strip()\n",
    "                     #   print(\"Stock: \"+stock)\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"short-description\"})\n",
    "                    #print(len(description_container))\n",
    "\n",
    "                    if len(description_container)!=0:\n",
    "                        description=description_container[0].p.text\n",
    "                        print(\"Description: \"+ str(description))\n",
    "\n",
    "\n",
    "                        moredesc=description_container[0].p.next_element.next_element.next_element.next_element.next_element.next_element.next_element\n",
    "                        #print(moredesc)\n",
    "\n",
    "                        if isinstance(description_container[0].p.next_element, NavigableString):\n",
    "                            k=1\n",
    "                        else:\n",
    "                            type1=description_container[0].p.next_element.text.strip()\n",
    "                        #print(\"type: \"+type1)\n",
    "\n",
    "\n",
    "\n",
    "                        if (moredesc!=None):\n",
    "                            if (isinstance(moredesc, NavigableString)):\n",
    "                                k=1\n",
    "                            else:\n",
    "\n",
    "                                for strong in moredesc.findAll('strong'):\n",
    "                                    Info = strong.text.strip()\n",
    "                                    if (Info.startswith('Fabric')):\n",
    "                                        Fabric=strong.next_element.next_element.strip()\n",
    "\n",
    "                                    elif (Info.startswith('Color')):\n",
    "                                        Color=strong.next_element.next_element.strip()\n",
    "\n",
    "                                    else:\n",
    "                                        k=1\n",
    "\n",
    "\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "                    images_container=soup.findAll(\"div\",{\"class\":\"MagicToolboxSelectorsContainer no-magic-scroll\"})\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for image in images_container[0].findAll('a'):\n",
    "                            #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                            #image=div.a\n",
    "                            #for image in image_container:\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "                           # print(substring)\n",
    "\n",
    "                              #downloads file:\n",
    "                            response = requests.get(\"https:\"+substring)\n",
    "                            file = open(\"SF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content)\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"SF_\"+str(product_count)+\"_\"\n",
    "\n",
    "                            #stores files in google drive:\n",
    "#                             name=\"SF_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "#                             #path=r\"C:\\Users\\dell\\n\" + name\n",
    "#                             path=os.path.normpath('C:/Users/dell/'+name) \n",
    "#                             #print(path)\n",
    "\n",
    "                           \n",
    "#                             image_path=StoreonGoogleDrive(name,\"1SOgDeN-sZ6HFyDZxghUJuUq9M_O2vdUA\",path) #this function stores the image with that name from path on google drive\n",
    "#                             #print(image_path)\n",
    "\n",
    "#                             image = cv2.imread(name)\n",
    "#                             features = cd.describe(image)\n",
    "#                             feature_vectors.append([])\n",
    "#                             feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "#                             images_arr.append(image_path)\n",
    "                            image_count=image_count+1\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                 \n",
    "#                         #store data in db\n",
    "#                         id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                         print(id1)\n",
    "#                         image_count=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                product_count+=1\n",
    "                print(\"\")\n",
    "\n",
    "            page_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CambridgeScrape():\n",
    "    #get browser by giving url and last productid\n",
    "    browser=scrollBrowser(\"https://thecambridgeshop.com/collections/designer-shalwar-kameez\",'product-4673381531734')\n",
    "    \n",
    "    #accessing html tags\n",
    "    container=browser.find_elements_by_class_name('product-collection products-grid row')\n",
    "    image_count=1\n",
    "\n",
    "    divv=browser.find_elements_by_xpath(\"//div[@class='product-image image-swap']\")\n",
    "    product_count=0\n",
    "    \n",
    "    for d in divv:\n",
    "        #empty variabls before every iteration\n",
    "        price=\"\"\n",
    "        Fabric=\"\"\n",
    "        Color=\"\"\n",
    "        sale=\"\"\n",
    "        sale_price=\"\"\n",
    "        Description=\"\"\n",
    "        stock=\"\"\n",
    "\n",
    "\n",
    "        product_link=d.find_element_by_css_selector('a').get_attribute('href') #select all links \n",
    "        print(\"product Count: \"+str(product_count))\n",
    "\n",
    "        print(\"Product Link: \"+product_link)\n",
    "        if (product_count>0):\n",
    "            request = requests.get(product_link) #get request from link\n",
    "            if (request.status_code == 200): #if link exists\n",
    "                html=urlopen(product_link)\n",
    "                soup=BeautifulSoup(html, \"html.parser\") #use beautifulsoup to parse through items\n",
    "                product_name=soup.h1.text.strip() #get product name from heading \n",
    "\n",
    "                price_container=soup.findAll(\"div\",{\"class\":\"prices\"}) #get price of product\n",
    "                price=price_container[0].span.span.text.strip() \n",
    "\n",
    "                sale_container=soup.findAll(\"span\",{\"class\":\"price on-sale\"}) #get to know if product is on sale\n",
    "                if (sale_container):\n",
    "                    sale=True\n",
    "                    sale_price=sale_container[0].span.text.strip()\n",
    "                else:\n",
    "                    sale=False\n",
    "                    sale_price=\"\"\n",
    "\n",
    "                Type=\"Shalwar Kameez\" #specify type of product\n",
    "\n",
    "                description_container=soup.findAll(\"div\",{\"class\":\"short-description\"}) #store description\n",
    "                Description=description_container[0].text.strip()\n",
    "\n",
    "                color_container=soup.findAll(\"div\",{\"class\":\"tooltip\"}) #store color\n",
    "                if (color_container):\n",
    "                    Color=color_container[0].text.strip()\n",
    "\n",
    "                fabric_container=soup.findAll(\"div\",{\"data-option-index\":\"2\"}) #store fabric\n",
    "                if (fabric_container):\n",
    "                    Fabric=fabric_container[0].label.text.strip()\n",
    "                  \n",
    "                #get container of images of product\n",
    "                images_container=soup.findAll(\"div\",{\"class\":\"test product-img-box left-vertical-moreview vertical-moreview\"})\n",
    "               \n",
    "\n",
    "                if (images_container):\n",
    "                    images=images_container[0].div.div #get all images\n",
    "                    images_arr=[]\n",
    "                    feature_vectors=[]\n",
    "\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for div in images.findAll('div'): #iterate through all images\n",
    "                            image=div.a\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "\n",
    "                            response = requests.get(\"https:\"+substring) #access image through requests\n",
    "                            file = open(\"CM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content) #write image in our own file. It is downloaded\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"CM_\"+str(product_count)+\"_\" #id of product\n",
    "\n",
    "                            #stores files in google drive:\n",
    "#                             name=\"CM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "#                             path=os.path.normpath('C:/Users/dell/'+name) #access image from PC\n",
    "\n",
    "#                             image_path=StoreonGoogleDrive(name,\"1Fu2Y5Dev990ax0Ly3eetHgJyr9Ekkykc\",path) #this function stores the image with that name from path on google drive\n",
    "                           \n",
    "#                             #read image, calcualte feature vectors and append in 2D array of feature vectors\n",
    "#                             image = cv2.imread(name)\n",
    "#                             features = cd.describe(image)\n",
    "#                             feature_vectors.append([])\n",
    "#                             feature_vectors[image_count-1]=features\n",
    "                        \n",
    "#                             images_arr.append(image_path) #append images of product in image array\n",
    "                            image_count=image_count+1\n",
    "\n",
    "#                     #store all the variables as key value pairs in mongodb\n",
    "#                     db_collection=getDbCon(Brand,'Men')\n",
    "#                     id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                     print(id1)\n",
    "\n",
    "                    image_count=1\n",
    "                print(\"\")\n",
    "        product_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutfitterScrape():\n",
    "    OWomenScrape()\n",
    "    OMenScrape()\n",
    "    \n",
    "def OMenScrape():\n",
    "\n",
    "    db_collection=getDbCon('Outfitters','Men') #specify brand and gender to get db\n",
    "\n",
    "    #get browser by giving url and last productid\n",
    "    browser1=scrollBrowser(\"https://outfitters.com.pk/collections/new-men\",'4479509332048')\n",
    "\n",
    "    divv=browser1.find_elements_by_xpath(\"//div[@class='product-bottom']\")\n",
    "    print(len(divv))\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    for d in divv:\n",
    "        if (d.text):\n",
    "            product_link=d.find_element_by_css_selector('a').get_attribute('href')\n",
    "            print(\"product Count: \"+str(product_count))\n",
    "            print(\"Product Link: \"+product_link)\n",
    "            if (product_count>0):\n",
    "\n",
    "                price=\"\"\n",
    "                Fabric=\"\"\n",
    "                Color=\"\"\n",
    "                sale=\"\"\n",
    "                stock=\"\"\n",
    "                sale_price=\"\"\n",
    "                Description=\"\"\n",
    "                Type=\"\"\n",
    "                images_arr=[]\n",
    "                feature_vectors=[]\n",
    "\n",
    "                request = requests.get(product_link)\n",
    "                if (request.status_code == 200):\n",
    "                    html=urlopen(product_link)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                    product_name=soup.h1.text.strip()\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"prices\"})\n",
    "                    price=price_container[0].span.span.text.strip()\n",
    "\n",
    "                    sale_container=soup.findAll(\"span\",{\"class\":\"price on-sale\"})\n",
    "                    if (sale_container):\n",
    "                        sale=True\n",
    "                        sale_price=sale_container[0].span.text.strip()\n",
    "                    else:\n",
    "                        sale=False\n",
    "                        sale_price=\"\"\n",
    "\n",
    "                    color_container=soup.findAll(\"div\",{\"class\":\"Rcolor-header\"})\n",
    "                    if (color_container):\n",
    "                        Color=color_container[0].p.span.text.strip()\n",
    "\n",
    "\n",
    "                    fabric_container=soup.findAll(\"ul\",{\"data-mce-fragment\":\"1\"})\n",
    "                    if (fabric_container):\n",
    "                        if (fabric_container[0].li.text.strip().find(\"%\")==-1): #because fabric is given in percentages\n",
    "                            Fabric=\"\"\n",
    "                        else:\n",
    "                            Fabric=fabric_container[0].li.text.strip()\n",
    "\n",
    "                    images_container=soup.findAll(\"div\",{\"class\":\"col-md-6 product-photos\"})\n",
    "                    print(len(images_container))\n",
    "\n",
    "                    if (images_container):\n",
    "                        images=images_container[0].div.div.div\n",
    "                        #print(images)\n",
    "                        images_arr=[]\n",
    "                        feature_vectors=[]\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for div in images.findAll('div'):\n",
    "                            #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                            image=div.a\n",
    "                           # print(image)\n",
    "                            #for image in image_container:\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "        #                     print(substring)\n",
    "\n",
    "                            response = requests.get(\"https:\"+substring)\n",
    "                            file = open(\"OM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content)\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"OM_\"+str(product_count)+\"_\"\n",
    "\n",
    "                            #stores files in google drive:\n",
    "#                                 name=\"OM_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "#                                 #path=r\"C:\\Users\\dell\\n\" + name\n",
    "#                                 path=os.path.normpath('C:/Users/dell/'+name) \n",
    "#                                 #print(path)\n",
    "\n",
    "\n",
    "#                                 image_path=StoreonGoogleDrive(name,\"1b1ICrKiPTTyVqC4EI37BKKAh2cn8ypt4\",path) #this function stores the image with that name from path on google drive\n",
    "#                                 #print(image_path)\n",
    "\n",
    "#                                 image = cv2.imread(name)\n",
    "#                                 features = cd.describe(image)\n",
    "#                                 feature_vectors.append([])\n",
    "#                                 feature_vectors[image_count-1]=features\n",
    "\n",
    "#                                 images_arr.append(image_path)\n",
    "                            image_count=image_count+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                         #store data in db\n",
    "#                         id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                         print(id1)\n",
    "\n",
    "                    image_count=1\n",
    "\n",
    "\n",
    "\n",
    "        product_count+=1\n",
    "        print(\"\")\n",
    "\n",
    "def OWomenScrape():\n",
    "\n",
    "    db_collection=getDbCon('Outfitters','Women') #specify brand and gender to get db\n",
    "\n",
    "    #get browser by giving url and last productid\n",
    "    browser1=scrollBrowser(\"https://outfitters.com.pk/collections/new-in-women-1\",'4479509332048')\n",
    "\n",
    "    divv=browser1.find_elements_by_xpath(\"//div[@class='product-bottom']\")\n",
    "    print(len(divv))\n",
    "    product_count=1\n",
    "    image_count=1\n",
    "    for d in divv:\n",
    "        if (d.text):\n",
    "            product_link=d.find_element_by_css_selector('a').get_attribute('href')\n",
    "            print(\"product Count: \"+str(product_count))\n",
    "            print(\"Product Link: \"+product_link)\n",
    "            if (product_count>0):\n",
    "\n",
    "                price=\"\"\n",
    "                Fabric=\"\"\n",
    "                Color=\"\"\n",
    "                sale=\"\"\n",
    "                stock=\"\"\n",
    "                sale_price=\"\"\n",
    "                Description=\"\"\n",
    "                Type=\"\"\n",
    "                images_arr=[]\n",
    "                feature_vectors=[]\n",
    "\n",
    "                request = requests.get(product_link)\n",
    "                if (request.status_code == 200):\n",
    "                    html=urlopen(product_link)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                    product_name=soup.h1.text.strip()\n",
    "\n",
    "                    price_container=soup.findAll(\"div\",{\"class\":\"prices\"})\n",
    "                    price=price_container[0].span.span.text.strip()\n",
    "\n",
    "                    sale_container=soup.findAll(\"span\",{\"class\":\"price on-sale\"})\n",
    "                    if (sale_container):\n",
    "                        sale=True\n",
    "                        sale_price=sale_container[0].span.text.strip()\n",
    "                    else:\n",
    "                        sale=False\n",
    "                        sale_price=\"\"\n",
    "\n",
    "                    color_container=soup.findAll(\"div\",{\"class\":\"Rcolor-header\"})\n",
    "                    if (color_container):\n",
    "                        Color=color_container[0].p.span.text.strip()\n",
    "\n",
    "\n",
    "                    fabric_container=soup.findAll(\"ul\",{\"data-mce-fragment\":\"1\"})\n",
    "                    if (fabric_container):\n",
    "                        if (fabric_container[0].li.text.strip().find(\"%\")==-1): #because fabric is given in percentages\n",
    "                            Fabric=\"\"\n",
    "                        else:\n",
    "                            Fabric=fabric_container[0].li.text.strip()\n",
    "\n",
    "                    images_container=soup.findAll(\"div\",{\"class\":\"col-md-6 product-photos\"})\n",
    "                    print(len(images_container))\n",
    "\n",
    "                    if (images_container):\n",
    "                        images=images_container[0].div.div.div\n",
    "                        #print(images)\n",
    "                        images_arr=[]\n",
    "                        feature_vectors=[]\n",
    "\n",
    "                    if len(images_container)!=0:\n",
    "                        for div in images.findAll('div'):\n",
    "                            #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                            image=div.a\n",
    "                           # print(image)\n",
    "                            #for image in image_container:\n",
    "                            url=image['href']   #gets the image but with extra characters\n",
    "                            split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                            substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "        #                     print(substring)\n",
    "\n",
    "                            response = requests.get(\"https:\"+substring)\n",
    "                            file = open(\"OW_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                            file.write(response.content)\n",
    "                            file.close()\n",
    "\n",
    "                            id_name=\"OW_\"+str(product_count)+\"_\"\n",
    "\n",
    "                            #stores files in google drive:\n",
    "#                                 name=\"OW_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "#                                 #path=r\"C:\\Users\\dell\\n\" + name\n",
    "#                                 path=os.path.normpath('C:/Users/dell/'+name) \n",
    "#                                 #print(path)\n",
    "\n",
    "\n",
    "#                                 image_path=StoreonGoogleDrive(name,\"1QAfByRxQfknB0KO_ELsbsG-U6zRHyyWj\",path) #this function stores the image with that name from path on google drive\n",
    "#                                 #print(image_path)\n",
    "\n",
    "#                                 image = cv2.imread(name)\n",
    "#                                 features = cd.describe(image)\n",
    "#                                 feature_vectors.append([])\n",
    "#                                 feature_vectors[image_count-1]=features\n",
    "\n",
    "#                                 images_arr.append(image_path)\n",
    "                            image_count=image_count+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                         #store data in db\n",
    "#                         id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                         print(id1)\n",
    "\n",
    "                    image_count=1\n",
    "\n",
    "\n",
    "\n",
    "        product_count+=1\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LimelightScrape():\n",
    "    db_collection=getDbCon('Limelight','Women') #specify brand and gender to get db\n",
    "    \n",
    "   \n",
    "    browser=scrollBrowser(\"https://www.limelight.pk/collections/shirt\",'prod-1339407499352')\n",
    "        \n",
    "    \n",
    "    condition=True\n",
    "    container=browser.find_elements_by_class_name('product-grid-3')\n",
    "\n",
    "\n",
    "    image_count=1\n",
    "    divv=browser.find_elements_by_xpath(\"//div[@class='ci alche-ci']\")\n",
    "    print(len(divv))\n",
    "    product_count=0\n",
    "    for d in divv:\n",
    "        if (d.text):\n",
    "\n",
    "            product_name=d.text\n",
    "            product_link=d.find_element_by_css_selector('a').get_attribute('href')\n",
    "            print(\"product Count: \"+str(product_count))\n",
    "            print(\"Product Link: \"+product_link)\n",
    "            product_count+=1\n",
    "            if (product_count>0):\n",
    "\n",
    "                price=\"\"\n",
    "                Fabric=\"\"\n",
    "                Color=\"\"\n",
    "                sale=\"\"\n",
    "                sale_price=\"\"\n",
    "                Description=\"\"\n",
    "                Type=\"\"\n",
    "                stock=\"\"\n",
    "\n",
    "                request = requests.get(product_link)\n",
    "                if (request.status_code == 200):\n",
    "                    html=urlopen(product_link)\n",
    "                    soup=BeautifulSoup(html, \"html.parser\")\n",
    "                    product_name=soup.h1.text.strip()\n",
    "\n",
    "\n",
    "                    sale_container=soup.findAll(\"span\",{\"class\":\"was\"})\n",
    "                    if (sale_container):\n",
    "                        sale=True\n",
    "                        price=sale_container[0].span.text.strip()\n",
    "                        price_container=soup.findAll(\"span\",{\"class\":\"product-price\"})\n",
    "                        sale_price=price_container[0].span.text.strip()\n",
    "                    else:\n",
    "                        sale=False\n",
    "                        sale_price=\"\"\n",
    "                        price_container=soup.findAll(\"span\",{\"class\":\"product-price\"})\n",
    "                        if (price_container[0]!=None):\n",
    "                            price=price_container[0].span.text.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    description_container=soup.findAll(\"div\",{\"class\":\"product-discription\"})\n",
    "                    #print(len(description_container))\n",
    "                    if len(description_container)!=0:\n",
    "                        #color=description_container[0].table.tbody.tr.td.ul.li\n",
    "                        description=description_container[0].table.tbody.tr.td.ul\n",
    "\n",
    "\n",
    "                        k=1\n",
    "                        Desc=\"\"\n",
    "                        Color=\"\"\n",
    "                        Fabric=\"\"\n",
    "                        for li in description.findAll('li'):\n",
    "                            Info=li.text.strip()\n",
    "\n",
    "\n",
    "                            if (Info.startswith('Color')):\n",
    "                                Color=Info\n",
    "                            if (Info.startswith('Fabric')):\n",
    "                                #print(\"First IF\")\n",
    "                                Fabric=Info\n",
    "                            elif (Info.startswith('Model')):\n",
    "                                k+=1\n",
    "                                #print(\"SEcomd if\")\n",
    "                            else:\n",
    "                                Desc+=Info\n",
    "                                Desc+='\\n'\n",
    "\n",
    "                            k+=1\n",
    "\n",
    "                        Type=\"Shirt\"\n",
    "\n",
    "\n",
    "                        images_container=soup.findAll(\"div\",{\"class\":\"thumbnail-slider1\"})\n",
    "                        if (len(images_container)==0):\n",
    "                            images_container=soup.findAll(\"div\",{\"class\":\"desktop-12 thumbnail-gallery\"})\n",
    "\n",
    "                        images_arr=[]\n",
    "                        feature_vectors=[]\n",
    "\n",
    "                        if len(images_container)!=0:\n",
    "                            for div in images_container[0].findAll('div'):\n",
    "                                #image_container=images_container[0].findAll('a', {'href':re.compile('.jpg')})\n",
    "                                image=div.a\n",
    "                                #for image in image_container:\n",
    "                                url=image['href']   #gets the image but with extra characters\n",
    "                                split_string = url.split(\"?\", 1)  #splits string in 2 when '?' is read\n",
    "                                substring = split_string[0]        #uses 1st half of substring because 2nd half isn't useful\n",
    "    #                             print(substring)\n",
    "\n",
    "                                response = requests.get(\"https:\"+substring)\n",
    "                                file = open(\"LW_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\", \"wb\") #first argument is a sample filename that the image has when it is downloaded\n",
    "                                file.write(response.content)\n",
    "                                file.close()\n",
    "\n",
    "                                id_name=\"LW_\"+str(product_count)+\"_\"\n",
    "\n",
    "                                #stores files in google drive:\n",
    "#                                 name=\"LW_\"+str(product_count)+\"_\"+str(image_count)+\".jpg\"\n",
    "#                                 #path=r\"C:\\Users\\dell\\n\" + name\n",
    "#                                 path=os.path.normpath('C:/Users/dell/'+name) \n",
    "#                                 #print(path)\n",
    "\n",
    "                              \n",
    "#                                 image_path=StoreonGoogleDrive(name,\"13ivBACuP65K6JR1AILYBO2sPQP5-WyYw\",path) #this function stores the image with that name from path on google drive\n",
    "\n",
    "#                                 image = cv2.imread(name)\n",
    "#                                 features = cd.describe(image)\n",
    "#                                 feature_vectors.append([])\n",
    "#                                 feature_vectors[image_count-1]=features\n",
    "\n",
    "\n",
    "\n",
    "#                                 images_arr.append(image_path)\n",
    "                                image_count=image_count+1\n",
    "\n",
    "\n",
    "                            \n",
    "#                             #store data in db\n",
    "#                             id1=InsertInDB(db_collection,id_name,product_name,price,sale,sale_price,stock,Color,product_link,Fabric,Description,Type,images_arr,feature_vectors)\n",
    "#                             print(id1)\n",
    "\n",
    "                            image_count=1\n",
    "                        print(\"\")\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape all brands one by one\n",
    "JdotScrape()\n",
    "KhaadiScrape()\n",
    "SapphireScrape()\n",
    "CambridgeScrape()\n",
    "OutfitterScrape()\n",
    "LimelightScrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
